# Visualizing Common Metrics on Probability Distributions

## Basics

### Entropy

Lots of good opportunities to explain entropy visually. I want to show a binary tree and use it to derive the information entropy equation.

Specifically, I can show that $B_x = \log \frac{1}{p(x)}$ corresponds exactly to the minimum number of bits to distinguish that $x$ occurred vs. some other event. Visually, $x$ owns some leafs of a binary tree, and $B_x$ gives us the minimum depth in the tree after which we could be sure any further progression will reach an $x$ leaf.

Then entropy is just the expectation of this quantity.

### Relative Entropy

After identifying entropy as above, KL Divergence (aka. relative entropy) follows: it's just the expectation of the difference $B_x$ under $q(x)$ and $B_x$ under $p(x)$. We want to know how much further down $p$'s tree one must progress to be sure of an outcome generated by $q$. When $p$ and $q$ have similar trees, the answer is usually "not far" (ie. small divergence). Otherwise, when $p$ and $q$ don't correlate, the divergence is larger.  

At this point, I can hint at the fact that if $q$ generates events $x^*$ that aren't in $p$'s tree, then no progression in the tree will reach $x^*$, so the divergence is undefined.

### Alternate Perspective to Coding: Distributions of Mass

At this point, I can shift gears and explain the Earth Mover's concepts behind probability distributions. There are two main visualizations I want to show here:
1. A 2D manifold in 3D, with some funky metrics that let people play around with the idea of a "cost function." 
2. A 2D heatmap of a joint distribution and its marginals, where users can pick a slice in one marginal and see where it ends up in the other marginal under the induced transport plan. 

These would make it really easy to build up the mathematical formulation of W1 distance and point out how it isn't undefined where KL would be.


# Scaling to Higher Dimensions

This is the section that would have all the neat 3D graphs of convergence thought experiments. Ideally I could make two disjoint point clouds and run an animated optimization process to have one match the other. 

The three main points for the section would be:
1. KL Divergence fails to provide useful gradients for approximating low-dimensional manifolds in many typical cases.
2.Â Wasserstein Distance does not.
3. The VAE method of "fuzzing" the manifold can help KL Divergence converge at the cost of enforcing a Gaussian model on the data points.

## If KL doesn't work, why can we train GANs?

This section would be about addressing the difference between the GAN objective and exactly minimizing JSD. It would also be the place to introduce abstractions like $f$-divergences and IPMs. 

This is the content that is the most interesting to *me*, but I think it's harder to get good visualizations here and I want to avoid saturating this post with heavier math. I see this as the bonus content if I have time for it, so I'll wait until earlier content is developed to plan this out more seriously.

