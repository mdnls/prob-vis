<head>
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
    <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>
    <script data-main="article" src="require.js"></script>
    <script src="bundle.js"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
          tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
          TeX: {
              Macros: {
                  expected: ["{\\mathbb{E}[#1]}", 1],
                  expectedunder: ["{\\mathbb{E}_{#1}[#2]}", 2]
              }
          }
        });
    </script>
    <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>
    <link rel="stylesheet" type="text/css" href="styles.css">
</head>
<body>
    <div class="container">
        <h1>Statistical Distances and Their Implications to GAN Training</h1>
        <div class="atn">
            <p>I will construct the blog in this page. Some visual components have separate demo pages:
                <ul>
                    <li><a href="heatmap.html">Heatmap</a></li>
                    <li><a href="histogram.html">Histogram</a></li>
                </ul>
            </p>
        </div>

        <h2>
            Table of Contents
        </h1>
        <ul>
            <li><p>Entropy of a Probability Distribution</p></li>
            <li><p>Relative Entropy</p></li>
            <li><p>Transport Plans</p></li>
            <li><p>Earthmover's Distance</p></li>
            <li><p>Fitting "Nice" Distributions</p></li>
            <li><p>Fitting "Real" Distributions</p></li>
        </ul>

        <div id="intro">
            <h2>
                Introduction
            </h2>

            <p>
                Generative Adversarial Networks are a class of neural architectures that can produce high quality samples of structured, high dimensional data. In particular, GANs make the assumption that this data is controlled by a small number of "factors of variation," which themselves can be modelled with a simple parametric distribution. During training, they learn a map from the factors of variation to the data space.
            </p>

            <p>
                 At its core, the training process relies on defining some notion of *distance* between a partially trained model and a hypothetical "optimal model" that can connect the factors of variation to the data perfectly. Minimizing this distance implies bringing the two closer together, so that eventually, you have a nearly perfect model. The feasibility of this process at scale depends on how we choose to determine this distance – for example, we *could* decide that two models either have distance zero if they are exactly the same, or infinite distance otherwise. In this case, if we don't already know the optimal model, this distance is neither computable nor informative. 
            </p>

            <p>
                Instead, this article will explore some of the more creative ways to define distances for GAN training, and in particular it will focus on the differences between information theoretic distance functions and Earth Mover's distance. EM has recently been proposed as improvement to information theoretic distances which can improve training stability and output quality in regimes that are typical for common GAN applications, and so it is important to understand the intuition behind why this is the case.
            </p>
            <h3>What does it mean to have a Statistical Distance?</h3>
            <p>
                The GAN itself is a map, and since the input to this map is a random variable, so is its output. The data itself is also a random variable, since there are certain items that are fairly likely ("heavy rain" or "slight drizzles") and other outcomes that are definitely impossible ("raining cats and dogs"). So, to determine the distance between a given GAN and the optimal model, we're really asking about the distance between two probability distributions.
            </p>
            <p>
                As a simple example, we can consider distributions in the form of a histogram, where each column represents an event and the boxes in a column represent the relative likelihood of that event. One of the simplest ways to compare histograms is by Euclidean distance*:.
                $$D(x, y) = \sum_{b \in \text{bins}} \vert b_x - b_y \vert$$
            </p>
            <p>Here, we simply use the magnitude of difference between each bin to say how far apart the distributions are. In the figure, this corresponds to the length of each line above or below a bin.</p>
            
            <div id="chisqr-1" class="vis-inset col-12">
                <div class="row h-100">          
                    <svg id="chisqr-1-left-svg" class="col-6"></svg>
                    <svg id="chisqr-1-right-svg" class="col-6"></svg>
                </div>
            </div>

            <p>When each bin matches exactly, the distance is zero, since the distribution over bins is the same. More importantly, if we make a small change to distribution $X$, by taking an \orangebox for example, this distance will reflect that change and decrease. We can use the distance as a guide to iteratively update $X$ until it matches $Y$. This is the approach that GANs take when approximating $P_d$, the true distribution over data, by the generator distribution $P_\theta$. The parameters $\theta$ are like the boxes, and the statistical distance function is the guide for manipulating $\theta$ to make $P_\theta$ faithful to $P_d$. </p>

            <p>The Euclidean distance is good for illustrating this concept, but it isn't used in practice. The original GAN formulation was shown to reduce an upper bound of the Jenson Shannon divergence between $P_d$ and $P_\theta$, an information theoretic distance between distributions based on the idea of entropy. These GANs are infamous for their difficulty to train, and they typically require lots of fine tuning to be implemented successfully. Within the last two years, there has been increasing interest in Earth Mover's distance as a more stable alternative, and the rest of this post is about why.</p>
        </div>

        <div id="entropy">
            <h2>
                Entropy
            </h2>

            <p>Paragraph: In its original formulation, the GAN framework is shown to minimize JSD, a statistical distance based on entropy <cite>1</cite>.</p>
            <p>Paragraph: intuition behind entropy is that two distributions are similar if they can be discribed in similar ways </p>
            <p>Paragraph: crucially, entropy relates probability distributions to the notion of information content. Every event has a quantifiable information. Two distributions are similar, then, if the events they describe typically generate similar amounts of information. </p>
            <p>Paragraph: formulation: $- \sum p(x) \log p(x) = \expected{ \log \frac{1}{p(x)}}$ </p>
            <p><b>Show basic histogram, with interactive presets, + table of entropy</b></p>

            <p>
                <div id="entropy-ex" class="vis-inset col-12">
                    <div class="h-100">
                        <svg id="entropy-ex-active" class="col-12 h-100"></svg>
                    </div>
                </div>
                <button id="entropy-ex-low">Low Entropy</button>
                <button id="entropy-ex-med">Medium Entropy</button>
                <button id="entropy-ex-high">High Entropy</button>
                <p>$H$ = <span id="entropy-ex-val"></span></p>
            </p>
            <p>Paragraph: the idea behind this formulation is that information content relates to literal information, transferred over wire via bits</p>
            <p>Paragraph: using bits to represent events induces a decision tree. $N=\frac{1}{p(x)}$ tells us how many outcomes the tree must have, and $\log N$ is the depth of a tree with $N$ outcomes. This quantity is the information content of $x$.</p>
            <p>Paragraph: the formula gives us an optimal value, if all encodings are perfect and use the tree optimally. This isn't always the case</p>
            <p><b>Show the huffman code for an interactive tree along with its entropy and the optimal entropy.</b></p>
            
            <div id="entropy-interactive" class="vis-inset-long col-8 offset-2">
                <div id="entropy-ex-interactive" class="h-100">

                </div>
            </div>
            
            <h2>
                Cross Entropy
            </h2>

            <p>Paragraph: from the perspective of entropy, two distributions are similar if their events typically have the same info content</p>
            <p>Paragraph: Kullback Leibler formulation: $D_{KL} (P \vert \vert Q) = \sum p(x) \log \frac{p(x)}{q(x)} = $$\expectedunder{x \sim P}{N_p} - \expectedunder{x \sim P}{N_q}$</p>
            <p><b>Show histograms for $p(x)$ and $q(x)$, highlighting the path length difference for selected event</b></p>
        
            <div id="xentropy-interactive" class="col-12">
                <div id="xentropy-ex-interactive" class="h-100">
                    
                </div>
            </div>
            
            <p>Paragraph: there are many other ways to use this idea to create a distance between distributions: JSD, KL, XEnt, etc</p>
            <p>Paragraph: however, all of these share one problem: when $q(x)=0$, the tree for $q(x)$ assigns no path at all to $x$.</p>
            <p>Glue paragraph: instead, we can look at transport metrics [IPMs] which handle this issue more nicely</p>
        </div>

        <div id="transport">
            <h2>Transport</h2>

            <p>Paragraph: a second option for comparing distributions comes from the idea of transport. How much work do we have to do to turn one distribution into another?</p>
            <p><b>Show two distributions side by side with arrows indicating transport</b></p>

            <div id="transport-interactive" class="col-12 vis-inset">
                <div class="h-75">
                        <div id="transport-ex-interactive" class="h-100">
        
                        </div>
                </div>
            </div>
            <p>Paragraph: we can show the entire transport plan in bulk, by defining a joint distribution</p>
            <p><b>Show the heatmap for the same two distributions</b></p>

            <div id="transport-matrix-interactive" class="col-12 vis-inset-long" style="margin: 20px">
                <div id="transport-matrix-ex-interactive" class="h-100">

                </div>
            </div>
            <p>Transport cost: $\iint_{X \times Y} p(x, y) c(x, y) dx dy$ - ie. for rectangles of size $dx dy$, we transport $p(x, y)$ of them, and each costs $c(x, y)$</p> 
            <p>Wasserstein Distance: the minimum transport cost to get from $p$ to $q$.</p>
            <p>Paragraph: wasserstein distance is "more general" than entropy distances</p>
        </div>

        <div id="convergence">
            <h2>How does this distinction affect training of generative models?</h2>
            <p>Paragraph: in general, training a generative model is feasible under either distance</p>
            <p>Paragraph: For example, consider "nice" distributions:</p>
            <p><b>Show animation of 2D convergence when distributions have the same support</b></p>
            <p>Paragraph: however there are cases when entropy breaks:</p>
            <p><b>Show animation of 2D convergence when distributions don't have the same support</b></p>
            <p>Paragraph: in particular, low dimensional manifolds are a "worst case" for entropy measures</p>
            <p>Paragraph: images have this kind of distribution</p>
        </div>

        <div id="conclusion">
            <h2>Conclusion</h2>
        </div>

        <div id="citations">
            <ol>
                <li>Goodfellow Paper</li>
            </ol>
        </div>
    </div>
</body>